# A quick introduction to deep learning for beginners

This series is adapted from the doctoral course *Introduction to deep learning for beginners* I will teach next year at [Ecole Nationale Supérieure d'Arts et Métiers](https://artsetmetiers.fr/en) (Paris, France). Its aim is to help beginners gradually build their intuition about the internal mechanics of neural networks as well as providing them with a basic understanding of the mathematics they rely on. Some historical aspects will also be addressed. For that purpose, we will start with simple linear classifiers such as Rosenblatt's single layer perceptron or the logistic regression before moving on to fully connected neural networks and other widespread architectures such as convolutional neural networks or LSTM networks. Various other subjects, e.g. convex and non-convex optimization, the universal approximation theorem or technical and ethical good practices will also be addressed along the way. Because our aim is to help beginners understand the inner workings of deep learning algorithms, all of the implementations that will be presented rely essentially on SciPy and NumPy rather than on highly optimized libraries like TensorFlow, at least whenever possible.

As of today, this series includes the following posts :
- [Rosenblatt's perceptron, the first modern neural network](./Rosenblatt_perceptron)
- [The Perceptron Convergence Theorem](???)
- [Limits of Rosenblatt's perceptron, a pathway to its demise](???)
- [Adaptive Linear Neurons and the Delta Rule, improving over Rosenblatt's perceptron](???)
